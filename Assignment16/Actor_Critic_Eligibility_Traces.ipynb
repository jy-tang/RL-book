{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import TypeVar,Optional,Mapping,Sequence,Iterable, Iterator, Tuple, TypeVar, Dict, Callable,List\n",
    "from rl.markov_decision_process import Policy\n",
    "import math\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "from dataclasses import dataclass, replace\n",
    "from rl.markov_decision_process import FinitePolicy, TransitionStep\n",
    "from rl.function_approx import FunctionApprox"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Linear_Approx_pi():\n",
    "    feature_func: Callable[[S,A],Sequence[float]]\n",
    "    weight: Sequence[float]\n",
    "\n",
    "    def update(self, delta_weight:Sequence[float]):\n",
    "        return replace(self,weight = self.weight + delta_weight)\n",
    "\n",
    "    def evaluate(self,state:S, action:A)->float:\n",
    "        return np.dot(self.weight,self.feature_func(state,action))\n",
    "\n",
    "    def get_gradient(self,state:S,action:A)->Sequence[float]:\n",
    "        return self.feature_func(state,action)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Linear_Approx_V():\n",
    "    feature_func: Callable[[S],Sequence[float]]\n",
    "    weight: Sequence[float]\n",
    "\n",
    "    def update(self, delta_weight:Sequence[float]):\n",
    "        return replace(self,weight = self.weight + delta_weight)\n",
    "\n",
    "    def evaluate(self,state:S)->float:\n",
    "        return np.dot(self.weight,self.feature_func(state))\n",
    "\n",
    "    def get_gradient(self,state:S)->Sequence[float]:\n",
    "        return self.feature_func(state)\n",
    "\n",
    "\n",
    "def sample_from_policy(\n",
    "        state:S,\n",
    "        pi: Linear_Approx_pi,\n",
    "        actions: Mapping[S,Iterable[A]]\n",
    ") -> A:\n",
    "    d = {}\n",
    "    for action in actions[state]:\n",
    "        d[action] = pi(state,action)\n",
    "    d = Categorical(d)\n",
    "\n",
    "    return d.sample()\n",
    "\n",
    "\n",
    "def Actor_Critic_Eligibility_Trace(\n",
    "         feature_func_pi: Callable[[S,A],Sequence[float]],     # feature functions\n",
    "         feature_func_V:Callable[[S],Sequence[float]],\n",
    "         simulator: Callable[[S,A],Tuple[S,float]],\n",
    "         theta0: Sequence[float],\n",
    "         v0: Sequence[float],\n",
    "         actions: Mapping[S,Iterable[A]],\n",
    "\n",
    "         gamma: float,\n",
    "\n",
    "         learning_rate_v: Callable[[int],float],\n",
    "         learning_rate_theta: Callable[[int],float],\n",
    "\n",
    "         lambda_theta: float,\n",
    "         lambda_v: float,\n",
    "\n",
    "         d: int,\n",
    "\n",
    "         state_distribution: Distribution[S],\n",
    "         tolerance: float = 1e-6,\n",
    "         nstop: int = None\n",
    "         )->Iterator[Sequence[float]]:\n",
    "    \"\"\"\n",
    "    LSTD algorithm.\n",
    "    feature_func:S->R^d. feature_func(terminal) = 0\n",
    "    simulator: Take input state and action, output next state and reward\n",
    "    d: dimension of features\n",
    "    theta0: The initial parameters for policy\n",
    "    v0: The initial parameters for V\n",
    "\n",
    "    actions: allowed actions for each state\n",
    "\n",
    "    learning_rate_theta: learning rate for theta as a function of number of appearance of  a (state,action) pari\n",
    "    learning_Rate_v: learning rate for v as a function of number of appearance of  a (state,action) pari\n",
    "    lambda_theta: lambda for theta trace eligibitiy\n",
    "    lambda_v: lambda for v trace eligibility\n",
    "\n",
    "    return: Iterator of (pi, v)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializations\n",
    "    theta = theta0\n",
    "    v = v0\n",
    "\n",
    "    pi = Linear_Approx_pi(feature_func = feature_func_pi, weight = theta)\n",
    "    value = Linear_Approx_V(feature_func = feature_func_V, weight = v)\n",
    "    max_steps = round(math.log(tolerance) / math.log(gamma)) if gamma < 1 else nstop\n",
    "\n",
    "    trace_count = 0\n",
    "    state_action_count = {}\n",
    "    while True:\n",
    "        state = state_distribution.sample()\n",
    "        z_theta = np.zeros(d)\n",
    "        z_v = np.zeros(d)\n",
    "        P = 1\n",
    "        trace_count += 1\n",
    "        # for each step in a episode\n",
    "        step_count = 0\n",
    "        while step_count < max_steps:\n",
    "            step_count += 1\n",
    "\n",
    "            action = sample_from_policy(state,pi,actions)\n",
    "\n",
    "            state_action_count[(state,action)]  = state_action_count.get((state,action),0.) +1\n",
    "\n",
    "            next_state,reward = simulator(state,action)\n",
    "\n",
    "            delta = reward + gamma*value.evaluate(next_state) - value.evaluate(state)\n",
    "\n",
    "            z_v = gamma*lambda_v*z_v + value.get_gradient(state)\n",
    "            z_theta = gamma*lambda_theta*z_theta + P*pi.get_gradient(state,action)/pi.evaluate(state,action)\n",
    "\n",
    "            alpha_v = learning_rate_v(state_action_count[(state,action)])\n",
    "            alpha_theta = learning_rate_theta(state_action_count[(state,action)])\n",
    "\n",
    "            delta_v = alpha_v*delta*z_v\n",
    "            delta_theta = alpha_theta*delta*z_theta\n",
    "\n",
    "            pi.update(delta_theta)\n",
    "            value.update(delta_v)\n",
    "\n",
    "            P = gamma*P\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        yield pi,value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}