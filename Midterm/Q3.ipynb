{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "3.1 The state space is defined as $\\mathcal{S} = \\{w|1 \\leq w \\leq W\\}$, which is the wage earned at a certain day.\n",
    "The action space is  $\\mathcal{A} = \\{(l,s)|0\\leq l \\leq H, 0\\leq s\\leq H-l\\}$, which is the learning and job searching hours at this day.\n",
    "The reward is defined as the total wage earned the day before this day $R(w',(l,s),w) = w*(H-l-s)$.\n",
    "\n",
    "To get the state transition probability, we consider three different cases.\n",
    "\n",
    "1. No new job offer\n",
    "2. There is a new job offer but the salary is lower than the current job\n",
    "3. THere is a new job offer and the salary is higher that the current job\n",
    "\n",
    "$$Pr[w'|w,a =(l,s)] = \\begin{cases} (1-\\beta s/H) \\times Poisson(\\mu = \\alpha l,k=x)& for \\ w' = min(w+x,W) \\\\ \\beta s/H \\times Poisson(\\mu = \\alpha l,k=0) & for \\ w' = min(w+1,W) \\\\ beta s/H \\times Poisson(\\mu = \\alpha l,k=x) & for \\ w' = min(w+x,W), x \\geq 1 \\end{cases}$$\n",
    "\n",
    "We group the same terms we have that\n",
    "\n",
    "$$Pr[w'|w,a =(l,s)] = \\begin{cases} (1-\\beta s/H) \\times Poisson(\\mu = \\alpha l,k=0)&  \\ w' = w \\\\ \\beta s/H \\times Poisson(\\mu = \\alpha l,k=0) +  Poisson(\\mu = \\alpha l,k=1)&  \\ w' = min(w+1,W) \\\\ Poisson(\\mu = \\alpha l,k=x) & \\ w' = min(w+x,W), x \\geq 2 \\end{cases}$$\n",
    "\n",
    "Especailly for edge case $w = W$ and $w = W -1$,\n",
    "\n",
    "$$Pr[w'=W|w=W,a = (l,s)] = 1 $$\n",
    "$$Pr[w'=W-1|w = W-1,a = (l,s)] = (1-\\beta s/H)Poisson(\\mu = \\alpha l,k=0) $$\n",
    "$$Pr[w'=W|w = W-1,a = (l,s)] = \\beta s/H \\times Poisson(\\mu = \\alpha l,k=0) + 1- Poisson(\\mu = \\alpha l,k=0)$$\n",
    "\n",
    "The discount factor is $\\gamma \\in [0,1)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.2 The MDP are implemented below"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "from scipy.stats import poisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# The state class\n",
    "@dataclass(frozen=True)\n",
    "class CareerState:\n",
    "    w: int\n",
    "\n",
    "# The aciton clasee\n",
    "@dataclass(frozen=True)\n",
    "class Action:\n",
    "    s: int\n",
    "    l: int\n",
    "\n",
    "CareerMapping = StateActionMapping[CareerState, Action]\n",
    "\n",
    "class CareerMDP(FiniteMarkovDecisionProcess[CareerState, Action]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        H: int,\n",
    "        W: int,\n",
    "        alpha: float,\n",
    "        beta: float\n",
    "    ):\n",
    "        self.H: int = H\n",
    "        self.W: int = W\n",
    "        self.alpha: float= alpha\n",
    "        self.beta: float = beta\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> CareerMapping:\n",
    "        d: Dict[CareerState, Dict[Action, Categorical[Tuple[CareerState,\n",
    "                                                            float]]]] = {}\n",
    "        for w in range(1,self.W+1):\n",
    "            state: CareerState = CareerState(w)\n",
    "            d1: Dict[Action, Categorical[Tuple[CareerState, float]]] = {}\n",
    "\n",
    "            for s in range(self.H+1):\n",
    "                for l in range(self.H-s+1):\n",
    "                    action = Action(s,l)\n",
    "                    if w == self.W:\n",
    "                        sr_probs_dict: Dict[Tuple[CareerState, float], float] =\\\n",
    "                                    {(CareerState(self.W),w*(self.H-l-s)):1}\n",
    "                    elif w == self.W-1:\n",
    "                        # probability that wp = self.W-1\n",
    "                        p1 = (1-self.beta*s/self.H)*poisson.pmf(k=0,mu=self.alpha*l)  # probability that wp = self.W-1\n",
    "                        # probability that wp = self.W\n",
    "                        p2 = self.beta*s/self.H*poisson.pmf(k=0,mu=self.alpha*l) +1-poisson.cdf(k=0,mu=self.alpha*l)\n",
    "                        sr_probs_dict: Dict[Tuple[CareerState, float], float] =\\\n",
    "                                {(CareerState(self.W-1),w*(self.H-l-s)):p1}\n",
    "                        sr_probs_dict[(CareerState(self.W),w*(self.H-l-s))] = p2\n",
    "                    elif w < self.W -1:\n",
    "                         # probability that wp = w\n",
    "                        p0 = (1-self.beta*s/self.H)*poisson.pmf(k=0,mu=self.alpha*l)\n",
    "                        # probability that wp = w+1\n",
    "                        p1 = self.beta*s/self.H*poisson.pmf(k=0,mu=self.alpha*l) + poisson.pmf(k=1,mu=self.alpha*l)\n",
    "                        sr_probs_dict: Dict[Tuple[CareerState, float], float] =\\\n",
    "                             {(CareerState(w),w*(self.H-l-s)):p0,\\\n",
    "                             (CareerState(w+1),w*(self.H-l-s)):p1}\n",
    "                        for x in range(2,self.W-w):\n",
    "                            sr_probs_dict[(CareerState(w+x),w*(self.H-l-s))] = poisson.pmf(k=x,mu=self.alpha*l)\n",
    "                        sr_probs_dict[(CareerState(self.W),w*(self.H-l-s))] = 1-poisson.cdf(k = self.W-w-1,mu=self.alpha*l)\n",
    "\n",
    "                    d1[action] = Categorical(sr_probs_dict)\n",
    "            d[state] = d1\n",
    "        return d\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.3,3.4\n",
    "Solve for the optimal value function and optimal policy using Iterations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Transition Map\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "si_mdp2: FiniteMarkovDecisionProcess[CareerState, str] =\\\n",
    "        CareerMDP(\n",
    "            H=10,\n",
    "            W =30,\n",
    "            alpha = 0.08,\n",
    "            beta = 0.82\n",
    "        )\n",
    "\n",
    "print(\"MDP Transition Map\")\n",
    "print(\"------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{CareerState(w=1): 1259.6504926227421, CareerState(w=2): 1340.415028776917, CareerState(w=3): 1426.3579138423866, CareerState(w=4): 1517.8111660380023, CareerState(w=5): 1615.128091467472, CareerState(w=6): 1718.684649031722, CareerState(w=7): 1828.8809028830492, CareerState(w=8): 1946.1425677166949, CareerState(w=9): 2070.9226507164517, CareerState(w=10): 2203.703212047008, CareerState(w=11): 2344.9974308597966, CareerState(w=12): 2495.3513213053984, CareerState(w=13): 2655.3256532742776, CareerState(w=14): 2825.6334066075733, CareerState(w=15): 3006.9962764014304, CareerState(w=16): 3199.999895219283, CareerState(w=17): 3399.9998886704875, CareerState(w=18): 3599.999882121693, CareerState(w=19): 3799.9998755728984, CareerState(w=20): 3999.9998690241036, CareerState(w=21): 4199.999862475308, CareerState(w=22): 4399.999855926513, CareerState(w=23): 4599.9998493777175, CareerState(w=24): 4799.999842828925, CareerState(w=25): 4999.99983628013, CareerState(w=26): 5199.999829731335, CareerState(w=27): 5399.999823182539, CareerState(w=28): 5599.9998166337455, CareerState(w=29): 5799.999810084951, CareerState(w=30): 5999.999803536155}\n",
      "For State CareerState(w=1):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=2):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=3):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=4):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=5):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=6):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=7):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=8):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=9):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=10):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=11):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=12):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=13):\n",
      "  Do Action Action(s=0, l=10) with Probability 1.000\n",
      "For State CareerState(w=14):\n",
      "  Do Action Action(s=10, l=0) with Probability 1.000\n",
      "For State CareerState(w=15):\n",
      "  Do Action Action(s=10, l=0) with Probability 1.000\n",
      "For State CareerState(w=16):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=17):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=18):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=19):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=20):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=21):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=22):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=23):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=24):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=25):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=26):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=27):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=28):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=29):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "For State CareerState(w=30):\n",
      "  Do Action Action(s=0, l=0) with Probability 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "opt_vf_vi2, opt_policy_vi2 = value_iteration_result(si_mdp2, gamma=0.95)\n",
    "print(opt_vf_vi2)\n",
    "print(opt_policy_vi2)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the optimal policy tells us that when $w \\leq 13$, one should use all $H$ hours to learn. When $14 \\leq w \\leq 15$, one should use all time to search for a new job. When $w \\geq 16$, one should use all the time to work.\n",
    "\n",
    "An intuitive explanation is that when we have very high wage, we prefer to put time in working to get higher overall pay. An extreme case is when we hae $w= W$, we should never learn or search.\n",
    "When the wage is very low, we should try to increase the hourly wage or find a new job with higher pay. Although the average pay rise when we spend all time in searching is a bit higher than learning (0.82 vs 0.8), learning gives us opportunity to achieve higher pay ($x>1$).\n",
    "So in order to optimize the career, when wage is low, we spend all the time learning. And we spend all the time searching for new job when the wage is in the middle range.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}