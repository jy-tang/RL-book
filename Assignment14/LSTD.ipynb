{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Sequence,Iterable, Iterator, Tuple, TypeVar, Dict, Callable,List\n",
    "from rl.markov_decision_process import Policy\n",
    "import math\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "def LSTD(feature_func: Callable[[S],Sequence[float]],     # feature functions\n",
    "         simulator: Callable[[S,A],Tuple[S,float]],\n",
    "         p:Policy[S,A],\n",
    "         d: int,\n",
    "         gamma: float,\n",
    "         epsilon: float,\n",
    "         state_distribution: Distribution[S],\n",
    "         tolerance: float = 1e-6,\n",
    "         nstop: int = None\n",
    "         )->Iterator[Sequence[float]]:\n",
    "    \"\"\"\n",
    "    LSTD algorithm.\n",
    "    feature_func:S->R^d. feature_func(terminal) = 0\n",
    "    simulator: Take input state and action, output next state and reward\n",
    "    d: dimension of features\n",
    "    p: The fixed policy\n",
    "    epsilon: small number for initialization\n",
    "\n",
    "    return: Iterator of weights R^d\n",
    "    \"\"\"\n",
    "\n",
    "    A_inverse = 1/epsilon*np.eye(d)\n",
    "    b = np.zeros(d)\n",
    "\n",
    "    max_steps = round(math.log(tolerance) / math.log(gamma)) if gamma < 1 else nstop\n",
    "\n",
    "    while True:\n",
    "        state = state_distribution.sample()\n",
    "        x = feature_func(state)\n",
    "\n",
    "        step_count = 0\n",
    "        while step_count < max_steps:\n",
    "            step_count += 1\n",
    "            action = p.act(state).sample()\n",
    "            next_state,reward = simulator(state,action)\n",
    "            xp = feature_func(next_state)\n",
    "\n",
    "            # update A^(-1), b, weight for each time step. Use Shermannm-Morrison incremental inverse\n",
    "            v = np.dot(np.transpose(A_inverse),x-gamma*xp)\n",
    "            A_inverse -= np.outer(np.dot(A_inverse,x),v)/(1+np.dot(v,x))\n",
    "            b += reward*x\n",
    "            weight = np.dot(A_inverse,b)\n",
    "\n",
    "            state = next_state\n",
    "            x = xp\n",
    "\n",
    "        yield weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}