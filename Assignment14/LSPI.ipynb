{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional,Mapping,Sequence,Iterable, Iterator, Tuple, TypeVar, Dict, Callable,List\n",
    "from rl.markov_decision_process import Policy\n",
    "import math\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "from dataclasses import dataclass, replace\n",
    "from rl.markov_decision_process import FinitePolicy, TransitionStep\n",
    "from rl.function_approx import FunctionApprox"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Linear_Approx_Q():\n",
    "    feature_func: Callable[[S,A],Sequence[float]]\n",
    "    weight: Sequence[float]\n",
    "\n",
    "    def update(self, new_weight:Sequence[float]):\n",
    "        return replace(self,weight = new_weight)\n",
    "\n",
    "    def evaluate(self,state:S, action:A)->float:\n",
    "        return np.dot(self.weight,self.feature_func(state,action))\n",
    "\n",
    "def policy_from_q(\n",
    "        q: Linear_Approx_Q,\n",
    "        actions: Mapping[S,Iterable[A]],\n",
    "        ϵ: float = 0.0\n",
    ") -> Policy[S, A]:\n",
    "\n",
    "    explore = Bernoulli(ϵ)\n",
    "\n",
    "    class QPolicy(Policy[S, A]):\n",
    "        def act(self, s: S) -> Optional[Distribution[A]]:\n",
    "            #terminal state?\n",
    "\n",
    "            if explore.sample():\n",
    "                return Choose(set(actions))\n",
    "\n",
    "            ind = np.argmax(q.evaluate([(s, a) for a in actions[s]]))\n",
    "            return Constant(actions[ind])\n",
    "\n",
    "    return QPolicy()\n",
    "\n",
    "def LSPI(feature_func: Callable[[S,A],Sequence[float]],     # feature functions\n",
    "         simulator: Callable[[S,A],Tuple[S,float]],\n",
    "         w0: Sequence[float],\n",
    "         actions: Mapping[S,Iterable[A]],\n",
    "         states: Sequence[S],\n",
    "         d: int,\n",
    "         gamma: float,\n",
    "         epsilon: float,\n",
    "         state_distribution: Distribution[S],\n",
    "         tolerance: float = 1e-6,\n",
    "         nstop: int = None\n",
    "         )->Iterator[Sequence[float]]:\n",
    "    \"\"\"\n",
    "    LSTD algorithm.\n",
    "    feature_func:S->R^d. feature_func(terminal) = 0\n",
    "    simulator: Take input state and action, output next state and reward\n",
    "    d: dimension of features\n",
    "    p0: The initial policy\n",
    "    w0: R_d, initial weight\n",
    "    epsilon: small number for initialization A\n",
    "    actions: allowed actions for each state\n",
    "\n",
    "    return: Iterator of weights R^d\n",
    "    \"\"\"\n",
    "\n",
    "    # initializations\n",
    "    A_inverse = 1/epsilon*np.eye(d)\n",
    "    b = np.zeros(d)\n",
    "    weight = w0\n",
    "    q = Linear_Approx_Q(feature_func = feature_func, weight = weight)\n",
    "    max_steps = round(math.log(tolerance) / math.log(gamma)) if gamma < 1 else nstop\n",
    "\n",
    "    trace_count = 0\n",
    "\n",
    "    while True:\n",
    "        state = state_distribution.sample()\n",
    "        trace_count += 1\n",
    "        e2 = 1/trace_count\n",
    "        # for each step in a episode\n",
    "        step_count = 0\n",
    "        while step_count < max_steps:\n",
    "            step_count += 1\n",
    "            p = policy_from_q(q,e2,actions)\n",
    "            action = p.act(state).sample()\n",
    "            x = feature_func(state,action)\n",
    "            next_state,reward = simulator(state,action)\n",
    "\n",
    "            # the off policy next action\n",
    "            ind = np.argmax([q.evaluate(next_state,action) for action in actions[next_state]])\n",
    "            ap = actions[next_state][ind]\n",
    "\n",
    "            xp = feature_func(next_state,ap)\n",
    "\n",
    "            # update A^(-1), b, weight for each time step. Use Shermannm-Morrison incremental inverse\n",
    "            v = np.dot(np.transpose(A_inverse),x-gamma*xp)\n",
    "            A_inverse -= np.outer(np.dot(A_inverse,x),v)/(1+np.dot(v,x))\n",
    "            b += reward*x\n",
    "            weight = np.dot(A_inverse,b)\n",
    "\n",
    "            state = next_state\n",
    "            q = q.update(new_weight=weight)\n",
    "\n",
    "\n",
    "        yield q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}