{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional,Mapping,Sequence,Iterable, Iterator, Tuple, TypeVar, Dict, Callable,List\n",
    "from rl.markov_decision_process import Policy\n",
    "import math\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "from dataclasses import dataclass, replace\n",
    "from rl.markov_decision_process import FinitePolicy, TransitionStep\n",
    "from rl.function_approx import FunctionApprox"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Linear_Approx_TDC():\n",
    "    feature_func: Callable[[S,A],Sequence[float]]\n",
    "    weight: Sequence[float]\n",
    "    theta: Sequence[float]\n",
    "\n",
    "    def update_weight(self, delta_weight:Sequence[float]):\n",
    "        return replace(self,weight = self.weight + delta_weight)\n",
    "\n",
    "    def update_theta(self,delta_theta:Sequence[float]):\n",
    "        return replace(self,weight = self.theta + delta_theta)\n",
    "\n",
    "    def evaluate(self,state:S, action:A)->float:\n",
    "        return np.dot(self.weight,self.feature_func(state,action))\n",
    "\n",
    "    def evaluate_theta(self,state:S, action:A)->float:\n",
    "        return np.dot(self.theta,self.feature_func(state,action))\n",
    "\n",
    "def policy_from_q(\n",
    "        q: Linear_Approx_TDC,\n",
    "        actions: Mapping[S,Iterable[A]],\n",
    "        ϵ: float = 0.0\n",
    ") -> Policy[S, A]:\n",
    "\n",
    "    explore = Bernoulli(ϵ)\n",
    "\n",
    "    class QPolicy(Policy[S, A]):\n",
    "        def act(self, s: S) -> Optional[Distribution[A]]:\n",
    "            #terminal state?\n",
    "\n",
    "            if explore.sample():\n",
    "                return Choose(set(actions))\n",
    "\n",
    "            ind = np.argmax(q.evaluate([(s, a) for a in actions[s]]))\n",
    "            return Constant(actions[ind])\n",
    "\n",
    "    return QPolicy()\n",
    "\n",
    "def TDC(feature_func: Callable[[S,A],Sequence[float]],     # feature functions\n",
    "         simulator: Callable[[S,A],Tuple[S,float]],\n",
    "         w0: Sequence[float],\n",
    "         theta0: Sequence[float],\n",
    "         actions: Mapping[S,Iterable[A]],\n",
    "         gamma: float,\n",
    "         state_distribution: Distribution[S],\n",
    "         learning_rate_alpha: Callable[[int],float],\n",
    "         learning_rate_beta: Callable[[int],float],\n",
    "         tolerance: float = 1e-6,\n",
    "         nstop: int = None\n",
    "         )->Iterator[Sequence[float]]:\n",
    "    \"\"\"\n",
    "    TDC for linear function approx, off-policy control\n",
    "    feature_func:S->R^d. feature_func(terminal) = 0\n",
    "    simulator: Take input state and action, output next state and reward\n",
    "\n",
    "    p0: The initial policy\n",
    "    w0: R_d, initial weight\n",
    "\n",
    "    actions: allowed actions for each state\n",
    "\n",
    "    learning_rate_alpha: learning rate for weight as a function of number of appearance of  a (state,action) pari\n",
    "    learning_Rate_beta: learning rate for theta as a function of number of appearance of  a (state,action) pari\n",
    "\n",
    "    return: Iterator of weights R^d\n",
    "    \"\"\"\n",
    "\n",
    "    # initializations\n",
    "\n",
    "    weight = w0\n",
    "    theta = theta0\n",
    "    q = Linear_Approx_TDC(feature_func = feature_func, weight = weight, theta = theta)\n",
    "    max_steps = round(math.log(tolerance) / math.log(gamma)) if gamma < 1 else nstop\n",
    "\n",
    "    trace_count = 0\n",
    "    count_state = {}\n",
    "\n",
    "    while True:\n",
    "        state = state_distribution.sample()\n",
    "        trace_count += 1\n",
    "        e2 = 1/trace_count\n",
    "        # for each step in a episode\n",
    "        step_count = 0\n",
    "        while step_count < max_steps:\n",
    "            step_count += 1\n",
    "            p = policy_from_q(q,e2,actions)\n",
    "            action = p.act(state).sample()\n",
    "            next_state,reward = simulator(state,action)\n",
    "\n",
    "            count_state[(state,action)] = count_state.get((state,action),0.) + 1\n",
    "\n",
    "            # the off policy next action\n",
    "            ind = np.argmax([q.evaluate(next_state,action) for action in actions[next_state]])\n",
    "            ap = actions[next_state][ind]\n",
    "\n",
    "            phi = feature_func(state,action)\n",
    "            phi_p = feature_func(next_state,ap)\n",
    "\n",
    "            # update weight and theta\n",
    "            delta = reward + gamma*q.evaluate(next_state,ap) - q.evaluate(state,action)\n",
    "            alpha = learning_rate_alpha(count_state[(state,action)])\n",
    "            beta = learning_rate_beta(count_state[(state,action)])\n",
    "\n",
    "            delta_weight = alpha*delta*phi - alpha*gamma*phi_p*q.evaluate_theta(action, state)\n",
    "            delta_theta = beta*(delta - q.evaluate_theta(action,state))*phi\n",
    "\n",
    "            state = next_state\n",
    "            q = q.update_weight(delta_weight = delta_weight)\n",
    "            q = q.update_theta(delta_theta = delta_theta)\n",
    "\n",
    "\n",
    "        yield q\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}