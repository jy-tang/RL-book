{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.monte_carlo import mc_control\n",
    "from typing import Iterable, Iterator, Tuple, TypeVar, Callable, Optional, Dict,Mapping\n",
    "\n",
    "from rl.distribution import Distribution\n",
    "from rl.function_approx import FunctionApprox\n",
    "import rl.markov_process as mp\n",
    "import rl.markov_decision_process as markov_decision_process\n",
    "from rl.markov_decision_process import (MarkovDecisionProcess)\n",
    "from rl.returns import returns\n",
    "from rl.markov_decision_process import FinitePolicy, TransitionStep\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import simple_inventory_dmp_cap as an example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{InventoryState(on_hand=0, on_order=1): -27.66095021630144,\n",
      " InventoryState(on_hand=0, on_order=0): -34.89484576629397,\n",
      " InventoryState(on_hand=1, on_order=0): -28.660950216301437,\n",
      " InventoryState(on_hand=0, on_order=2): -27.991890076067463,\n",
      " InventoryState(on_hand=1, on_order=1): -28.991890076067467,\n",
      " InventoryState(on_hand=2, on_order=0): -29.991890076067463}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap, InventoryState\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "       SimpleInventoryMDPCap(\n",
    "           capacity=user_capacity,\n",
    "           poisson_lambda=user_poisson_lambda,\n",
    "           holding_cost=user_holding_cost,\n",
    "           stockout_cost=user_stockout_cost\n",
    "       )\n",
    "\n",
    "\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "pprint(opt_vf_vi)\n",
    "print(opt_policy_vi)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement Tabular MC control with GLIE, $\\epsilon = 1/k$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "# Define a helper function to get policy from Q\n",
    "def greedy_epsilon_policy(\n",
    "        q: Mapping[S, Mapping[A,float]],\n",
    "        epsilon: float\n",
    ")->FinitePolicy[S,A]:\n",
    "    #follow the structure of rl.markov_decision_process.policy_from_q, but restore policy in a map\n",
    "    policy_map:Mapping[S, Optional[FiniteDistribution[A]]] = {}\n",
    "\n",
    "    for state in q.keys():\n",
    "        actions = q[state].keys()\n",
    "        max_action = max(q[state], key=q[state].get)\n",
    "        d = {action:epsilon/len(actions) for action in actions}\n",
    "        d[max_action] += 1-epsilon\n",
    "        policy_map[state] = Categorical(d)\n",
    "\n",
    "    return FinitePolicy(policy_map)\n",
    "\n",
    "\n",
    "\n",
    "def mc_tabular_control(\n",
    "    simulator: Callable[[Distribution[S],FinitePolicy[S,A]], Iterator[TransitionStep[S, A]]], # given distribution of initial state,policy,return a trace iterator\n",
    "    state_distribution: Distribution[S],\n",
    "    gamma: float,\n",
    "    initial_q: Mapping[S, Mapping[A,float]],\n",
    "    initial_p: FinitePolicy[S,A],\n",
    "    tolerance: float = 1e-6\n",
    "\n",
    "):\n",
    "    #initialize q and p\n",
    "    q = initial_q\n",
    "    p = initial_p\n",
    "    count = {}           # record the number of appearance of (action,state) pair\n",
    "    trace_count = 0      # record the number of traces\n",
    "    while True:\n",
    "        trace_count += 1\n",
    "        epsilon = 1/trace_count\n",
    "        trace = simulator(state_distribution, p)\n",
    "        return_trace = returns(trace,gamma,tolerance)   # calculate return from trace\n",
    "\n",
    "        for step in return_trace:\n",
    "            count[(step.state,step.action)] = count.get((step.state,step.action),0.)+1\n",
    "            #update Q\n",
    "            q[step.state][step.action] += 1/count[(step.state,step.action)]*(step.return_ - q[step.state][step.action])\n",
    "\n",
    "        #update policy\n",
    "        p = greedy_epsilon_policy(q, epsilon)\n",
    "        yield q,p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run MC control algorithm on the simple inventory case. Use mdp.simulate_actions as the simulator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{InventoryState(on_hand=0, on_order=1): -27.68414929855416,\n",
      " InventoryState(on_hand=0, on_order=0): -34.93079434279971,\n",
      " InventoryState(on_hand=1, on_order=0): -28.689134875877556,\n",
      " InventoryState(on_hand=0, on_order=2): -28.594010741244826,\n",
      " InventoryState(on_hand=1, on_order=1): -29.021871164257433,\n",
      " InventoryState(on_hand=2, on_order=0): -30.02724869732018}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "  Do Action 2 with Probability 0.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_simulator = si_mdp.simulate_actions\n",
    "states = si_mdp.states()\n",
    "my_state_distribution =Categorical({state:1/len(states) for state in states})\n",
    "my_gamma = 0.9\n",
    "\n",
    "# initial q and p value\n",
    "my_initial_q = {state:{action: -2.0 for action in si_mdp.actions(state)} for state in states}\n",
    "my_initial_p = FinitePolicy({state:Categorical({action: 1/len(si_mdp.actions(state)) for action in si_mdp.actions(state)})for state in states})\n",
    "\n",
    "x = mc_tabular_control(\n",
    "    simulator = my_simulator, # given distribution of initial state,policy,return a trace iterator\n",
    "    state_distribution = my_state_distribution,\n",
    "    gamma = my_gamma,\n",
    "    initial_q = my_initial_q,\n",
    "    initial_p = my_initial_p\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "*_,(q_final,p_final) = itertools.islice(x,5000)\n",
    "V = {}\n",
    "for state in q_final.keys():\n",
    "    for action in q_final[state]:\n",
    "        V[state] = V.get(state,0.0) + p_final.act(state).probability(action)*q_final[state][action]\n",
    "\n",
    "pprint(V)\n",
    "print(p_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the intialization of Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{InventoryState(on_hand=0, on_order=1): -27.991239485068757,\n",
      " InventoryState(on_hand=0, on_order=0): -35.5316042017312,\n",
      " InventoryState(on_hand=1, on_order=0): -28.976309379523904,\n",
      " InventoryState(on_hand=0, on_order=2): -28.36273126802262,\n",
      " InventoryState(on_hand=1, on_order=1): -29.38442287203882,\n",
      " InventoryState(on_hand=2, on_order=0): -30.369751798800348}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 0.000\n",
      "  Do Action 2 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_simulator = si_mdp.simulate_actions\n",
    "states = si_mdp.states()\n",
    "my_state_distribution =Categorical({state:1/len(states) for state in states})\n",
    "my_gamma = 0.9\n",
    "\n",
    "# initial q and p value\n",
    "my_initial_q = {state:{action: -1.0 for action in si_mdp.actions(state)} for state in states}\n",
    "my_initial_p = FinitePolicy({state:Categorical({action: 1/len(si_mdp.actions(state)) for action in si_mdp.actions(state)})for state in states})\n",
    "\n",
    "x = mc_tabular_control(\n",
    "    simulator = my_simulator, # given distribution of initial state,policy,return a trace iterator\n",
    "    state_distribution = my_state_distribution,\n",
    "    gamma = my_gamma,\n",
    "    initial_q = my_initial_q,\n",
    "    initial_p = my_initial_p\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "*_,(q_final,p_final) = itertools.islice(x,5000)\n",
    "V = {}\n",
    "for state in q_final.keys():\n",
    "    for action in q_final[state]:\n",
    "        V[state] = V.get(state,0.0) + p_final.act(state).probability(action)*q_final[state][action]\n",
    "\n",
    "pprint(V)\n",
    "print(p_final)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the initialization value will result in different policy. Why?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}