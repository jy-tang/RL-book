{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rl.monte_carlo import mc_control\n",
    "from typing import Iterable, Iterator, Tuple, TypeVar, Callable, Optional, Dict,Mapping\n",
    "\n",
    "from rl.distribution import Distribution\n",
    "from rl.function_approx import FunctionApprox\n",
    "import rl.markov_process as mp\n",
    "import rl.markov_decision_process as markov_decision_process\n",
    "from rl.markov_decision_process import (MarkovDecisionProcess)\n",
    "from rl.returns import returns\n",
    "from rl.markov_decision_process import FinitePolicy, TransitionStep\n",
    "from rl.distribution import (Bernoulli, Constant, Categorical, Choose,\n",
    "                             Distribution, FiniteDistribution)\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import simple_inventory_dmp_cap as an example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{InventoryState(on_hand=1, on_order=0): -28.660950216301437,\n",
      " InventoryState(on_hand=0, on_order=1): -27.66095021630144,\n",
      " InventoryState(on_hand=0, on_order=2): -27.991890076067463,\n",
      " InventoryState(on_hand=0, on_order=0): -34.89484576629397,\n",
      " InventoryState(on_hand=1, on_order=1): -28.991890076067467,\n",
      " InventoryState(on_hand=2, on_order=0): -29.991890076067463}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap, InventoryState\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "       SimpleInventoryMDPCap(\n",
    "           capacity=user_capacity,\n",
    "           poisson_lambda=user_poisson_lambda,\n",
    "           holding_cost=user_holding_cost,\n",
    "           stockout_cost=user_stockout_cost\n",
    "       )\n",
    "\n",
    "\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "pprint(opt_vf_vi)\n",
    "print(opt_policy_vi)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement tabular SARSA with the function SARSA_tabular_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "# Define a helper function to get policy from Q\n",
    "def greedy_epsilon_policy(\n",
    "        q: Mapping[S, Mapping[A,float]],\n",
    "        epsilon: float\n",
    ")->FinitePolicy[S,A]:\n",
    "    #follow the structure of rl.markov_decision_process.policy_from_q, but restore policy in a map\n",
    "    policy_map:Mapping[S, Optional[FiniteDistribution[A]]] = {}\n",
    "\n",
    "    for state in q.keys():\n",
    "        actions = q[state].keys()\n",
    "        max_action = max(q[state], key=q[state].get)\n",
    "        d = {action:epsilon/len(actions) for action in actions}\n",
    "        d[max_action] += 1-epsilon\n",
    "        policy_map[state] = Categorical(d)\n",
    "\n",
    "    return FinitePolicy(policy_map)\n",
    "\n",
    "\n",
    "\n",
    "def Qlearning_tabular(\n",
    "    simulator: Callable[[S,A], TransitionStep[S, A]], # given distribution of initial state,policy,return a trace iterator\n",
    "    state_distribution: Distribution[S],\n",
    "    gamma: float,\n",
    "    initial_q: Mapping[S, Mapping[A,float]],\n",
    "    learning_rate_func: Callable[[int], float],\n",
    "    tolerance: float = 1e-6,\n",
    "    nstop: int = None\n",
    "\n",
    "\n",
    "):\n",
    "    #initialize q and p. In this case assume no terminal states\n",
    "    q = initial_q\n",
    "    #p = initial_p\n",
    "    count = {}           # record the number of appearance of (action,state) pair\n",
    "    trace_count = 0      # record the number of traces\n",
    "\n",
    "    max_steps = round(math.log(tolerance) / math.log(gamma)) if gamma < 1 else nstop\n",
    "\n",
    "    while True:    # for each  episode\n",
    "        trace_count += 1\n",
    "        epsilon = 1/trace_count\n",
    "\n",
    "        state = state_distribution.sample()      # initialize S\n",
    "\n",
    "        step_count = 0\n",
    "        while step_count < max_steps:\n",
    "            step_count += 1\n",
    "\n",
    "            \n",
    "            p = greedy_epsilon_policy(q,epsilon)     # Choose A from S using policy from Q\n",
    "            action = p.act(state).sample()\n",
    "            count[(state,action)] = count.get((state,action),0.)+1\n",
    "\n",
    "            next_state,reward = simulator(state,action)   # get next state and reward\n",
    "            p = greedy_epsilon_policy(q,epsilon)\n",
    "\n",
    "            #update Q\n",
    "            q[state][action] += learning_rate_func(count[state,action])*(reward + gamma*max(q[next_state].values())-q[state][action])\n",
    "            state = next_state\n",
    "\n",
    "        yield q,p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulate simple inventory with SARSA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{InventoryState(on_hand=1, on_order=0): -28.61970258219844,\n",
      " InventoryState(on_hand=0, on_order=1): -27.63213966504941,\n",
      " InventoryState(on_hand=0, on_order=2): -27.8579055432372,\n",
      " InventoryState(on_hand=0, on_order=0): -34.91922886200373,\n",
      " InventoryState(on_hand=1, on_order=1): -28.92941518317391,\n",
      " InventoryState(on_hand=2, on_order=0): -29.96029094945271}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "  Do Action 2 with Probability 0.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 0 with Probability 0.000\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.function_approx import learning_rate_schedule\n",
    "initial_learning_rate: float = 0.03\n",
    "half_life: float = 1000.0\n",
    "exponent: float = 0.5\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule( initial_learning_rate=initial_learning_rate, half_life=half_life, exponent=exponent)\n",
    "\n",
    "def simulator(state:S,action:A)->Tuple[S,A]:\n",
    "    return si_mdp.step(state,action).sample()\n",
    "\n",
    "states = si_mdp.states()\n",
    "state_distribution =Categorical({state:1/len(states) for state in states})\n",
    "initial_q = {state:{action: -3.0 for action in si_mdp.actions(state)} for state in states}\n",
    "\n",
    "x = Qlearning_tabular(\n",
    "    simulator=simulator, # given distribution of initial state,policy,return a trace iterator\n",
    "    state_distribution=state_distribution,\n",
    "    gamma=0.9,\n",
    "    initial_q=initial_q,\n",
    "    learning_rate_func = learning_rate_func,\n",
    ")\n",
    "\n",
    "import itertools\n",
    "*_,(q_final,p_final) = itertools.islice(x,10000)\n",
    "V = {}\n",
    "for state in q_final.keys():\n",
    "    for action in q_final[state]:\n",
    "        V[state] = V.get(state,0.0) + p_final.act(state).probability(action)*q_final[state][action]\n",
    "\n",
    "pprint(V)\n",
    "print(p_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see very good agreement with DP here for all kind of initialization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}