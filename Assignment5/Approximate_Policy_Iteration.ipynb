{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Mapping, Tuple, TypeVar, Sequence, List\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Distribution, Constant\n",
    "from rl.function_approx import FunctionApprox\n",
    "from rl.iterate import iterate\n",
    "from rl.markov_process import (FiniteMarkovRewardProcess, MarkovRewardProcess,\n",
    "                               RewardTransition)\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess, Policy,\n",
    "                                        MarkovDecisionProcess,\n",
    "                                        StateActionMapping)\n",
    "from rl.dynamic_programming import greedy_policy_from_vf\n",
    "from rl.approximate_dynamic_programming import evaluate_mrp\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "\n",
    "def policy_iteration(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    gamma: float,\n",
    "    approx_0: FunctionApprox[S],\n",
    "    non_terminal_states_distribution: Distribution[S],\n",
    "    num_state_samples: int,\n",
    "    num_policy_evaluation: int\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Iteratively calculate the Optimal Value function for the given\n",
    "    Markov Decision Process, using the given FunctionApprox to approximate the\n",
    "    Optimal Value function at each step for a random sample of the process'\n",
    "    non-terminal states.\n",
    "\n",
    "    '''\n",
    "    def update(v: FunctionApprox[S]) -> FunctionApprox[S]:\n",
    "        nt_states: Sequence[S] = non_terminal_states_distribution.sample_n(\n",
    "            num_state_samples\n",
    "        )\n",
    "\n",
    "\n",
    "        def return_(s_r: Tuple[S, float]) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + gamma * v.evaluate([s1]).item()\n",
    "\n",
    "        class greedy_policy(Policy[S, A]):\n",
    "            mdp: MarkovDecisionProcess\n",
    "            vf: FunctionApprox\n",
    "            gamma: float\n",
    "\n",
    "            def __init__(self, mdp: MarkovDecisionProcess,vf:FunctionApprox,gamma:float):\n",
    "                self.mdp = mdp\n",
    "                self.vf.vf\n",
    "                self.gamma = gamma\n",
    "\n",
    "            def act(self, s: S) -> Distribution[A]:\n",
    "                action =max([(a,self.mdp.step(s, a).expectation(return_,))\n",
    "                     for a in mdp.actions(s)],key = itemgetter(1))[0]\n",
    "                return Constant(action)\n",
    "\n",
    "        policy = greedy_policy(\n",
    "            mdp=mdp,\n",
    "            vf = v,\n",
    "            gamma = gamma\n",
    "            )\n",
    "        mrp = mdp.apply_policy(policy)\n",
    "        return itertools.islice(evaluate_mrp(mrp,gamma,v,non_terminal_states_distribution,num_state_samples),num_policy_evaluation)\n",
    "\n",
    "    return iterate(update, approx_0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}